# Audio-Visual fusion deep neural network for musical audio source separation

A repurposing [Audio-Visual Speech Enhancement Using Multimodal Deep Convolutional Neural Networks](https://arxiv.org/pdf/1703.10893), Hou Et Al. 2017, for musical audio source separation with an FFT-based approach of predicting a separated source's magnitude and phase FFT with video and audio input.

Trained on the [MUSICES dataset](https://arxiv.org/pdf/1910.10997.pdf), Zhou Et Al., 2020. Tools are available in this repo to download this dataset (available as YouTube videos.) The dataset was introduced along with [this repo](https://github.com/Hangz-nju-cuhk/Vision-Infused-Audio-Inpainter-VIAI) as an expansion of the MUSIC dataset, containing video and audio of musicians playing instruments from 11 classes.

## Colab training (must download and process dataset first)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1qLRYtIsAlyc73Ew62bAybjoAT2u8raID?usp=sharing)
